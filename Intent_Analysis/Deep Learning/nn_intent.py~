import json
from textblob import TextBlob
from nltk.stem.lancaster import LancasterStemmer
import re
import random
import numpy as np
import tflearn
import tensorflow as tf
import pickle
from load_embeddings import Embedding
#from document_embeddings import normal_averaging,averaging_tf_idf_features
stemmer = LancasterStemmer()

def load_intent_data(file_location):
	with open(file_location) as json_data:
    		intents = json.load(json_data)
	return intents

def get_data(intents):
	words = list()
	documents = list()
	for intent in intents['data']:
		for example in intents['data'][intent]['examples']:
			example = re.sub(pattern='(\W+)|(\d+)|(\s+)',repl=' ',string=example)
			blob = TextBlob(example.lower())
        		words.extend(blob.words) # here tokens are words of document
			documents.append((blob.words,intent))

	words = [stemmer.stem(w.lower()) for w in words]
	words = sorted(list(set(words)))

	classes = intents['data'].keys()

	return documents,words,classes

def word_embeddings(intents):

	#documents,words,classes = get_data(intents)
	"""Getting Features from normal word averaging"""
	#train_X,train_y = normal_averaging(documents,classes)
	#pickle.dump( {'words':words, 'classes':classes, 'train_x':train_X, 'train_y':train_y}, open( "training_data", "wb" ))
	data = pickle.load( open( "/home/abhishek/Facebook_data/CIO/Intent_Analysis/Deep Learning/training_data", "rb" ) )
	words = data['words']
	classes = data['classes']
	train_x = data['train_x']
	train_y = data['train_y']
	return train_x,train_y
def bag_of_words(intents):

	documents,words,classes = get_data(intents)
	training = []
	output = []
	# create an empty array for our output
	output_empty = [0] * len(classes)

	for doc in documents:
		# initialize our bag of words
		bag = []
		# list of tokenized words for the pattern
		pattern_words = doc[0]
		# stem each word
		pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]
		# create our bag of words array
		for w in words:
			bag.append(1) if w in pattern_words else bag.append(0)
		# output is a '0' for each tag and '1' for current tag
		output_row = list(output_empty)
		output_row[classes.index(doc[1])] = 1
		training.append([bag, output_row])

	random.shuffle(training)
	training = np.array(training)
	train_x = list(training[:,0])
	train_y = list(training[:,1])
	return train_x,train_y,words,classes

def train_nn(train_x,train_y):

	tf.reset_default_graph()
	# Build neural network
	net = tflearn.input_data(shape=[None, len(train_x[0])])
	net = tflearn.fully_connected(net, 8)
	net = tflearn.fully_connected(net, 8)
	net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')
	net = tflearn.regression(net)

	# Define model and setup tensorboard
	model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')
	# Start training (apply gradient descent algorithm)
	model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True)
	model.save('model.tflearn')

def neural_network_model_BOW():
	intents = load_intent_data(file_location)
	train_x,train_y,words,classes = bag_of_words(intents)
	train_nn(train_x,train_y)
	pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( "training_data", "wb" ))

def neural_network_model_w2v():
	intents = load_intent_data("intent.json")
	train_x , train_y = word_embeddings(intents)
	train_nn(train_x,train_y)
